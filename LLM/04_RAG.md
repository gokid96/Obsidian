

# RAG (Retrieval-Augmented Generation)

## 개념

검색(Retrieval) + 생성(Generation)을 합친 기술
LLM이 모르는 정보를 외부에서 검색해서 답변에 활용

## 왜 필요한가?

LLM의 한계:
- 학습 이후 정보는 모름 (지식 컷오프)
- 회사 내부 문서는 학습 안 됨
- 잘못된 정보를 자신있게 말함 (할루시네이션)

RAG로 해결:
- 최신 정보 검색해서 답변
- 회사 문서 기반 답변 가능
- 출처 있는 답변 → 신뢰도 상승

## 작동 방식
```

1. 사용자 질문 ↓
2. 관련 문서 검색 (벡터DB에서) ↓
3. 검색된 문서 + 질문을 LLM에 전달 ↓
4. LLM이 문서 기반으로 답변 생성

```

## 예시
```

사용자: "우리 회사 연차 정책이 뭐야?"

[RAG 없이] LLM: "일반적으로 연차는..." (모름)

[RAG 있을 때]

1. 검색: 사내 인사규정.pdf에서 관련 내용 찾음
2. LLM에 전달: "이 문서 참고해서 답변해"
3. 답변: "사내 규정에 따르면 연차는 15일이며..."

```

## 핵심 구성 요소

| 구성 요소 | 역할 | 예시 |
|-----------|------|------|
| 문서 저장소 | 원본 데이터 보관 | S3, 파일 시스템 |
| 임베딩 모델 | 텍스트 → 벡터 변환 | OpenAI Embedding, 한국어 모델 |
| 벡터DB | 벡터 저장 및 검색 | Pinecone, Chroma, Milvus |
| LLM | 최종 답변 생성 | GPT-4, Claude |

## 구현 흐름

### 1단계: 문서 준비 (인덱싱)
```

문서 수집 → 청크 분할 → 임베딩 변환 → 벡터DB 저장

```

- 청크 분할: 긴 문서를 작은 조각으로 나눔 (500~1000자)
- 임베딩: 텍스트를 숫자 벡터로 변환
- 저장: 벡터DB에 저장

### 2단계: 검색 및 답변
```

질문 임베딩 → 유사 문서 검색 → LLM에 전달 → 답변

```

## 주요 도구/프레임워크

| 도구 | 용도 |
|------|------|
| LangChain | RAG 파이프라인 구축 (가장 많이 씀) |
| LlamaIndex | 문서 인덱싱 특화 |
| Pinecone | 클라우드 벡터DB |
| Chroma | 로컬/오픈소스 벡터DB |
| Faiss | Meta의 벡터 검색 라이브러리 |

## 장점

- 최신 정보 반영 가능
- 회사 데이터 활용 가능
- 출처 표시 가능 → 신뢰도 상승
- 파인튜닝보다 구현 쉽고 저렴

## 단점

- 검색 품질에 따라 답변 품질 달라짐
- 벡터DB 운영 필요
- 청크 분할, 임베딩 최적화 필요

## 한 줄 요약

> "LLM에게 오픈북 시험 보게 하는 것"

