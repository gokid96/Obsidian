## 정의
텍스트를 모델이 처리할 수 있는 숫자(토큰 ID)로 변환하는 과정

## 예시
```
"안녕하세요" → [45678, 12345]
"Hello world" → [9906, 1917]
```

## 토큰 ≠ 글자 ≠ 단어
- 영어: 대략 1토큰 = 4글자 = 0.75단어
- 한글: 1토큰 = 1~2글자 (영어보다 비효율적)

## 주요 알고리즘
| 알고리즘 | 사용 모델 |
|----------|-----------|
| BPE (Byte Pair Encoding) | GPT, Llama |
| WordPiece | BERT |
| SentencePiece | T5, 다국어 모델 |
## BPE 원리
1. 모든 글자를 개별 토큰으로 시작
2. 가장 자주 붙어있는 쌍을 하나로 합침
3. 원하는 vocab 크기가 될 때까지 반복

## 왜 중요한가
- 토큰 수 = 비용 (API는 토큰당 과금)
- 컨텍스트 길이 제한도 토큰 기준
- 한글이 영어보다 토큰 많이 먹음 → 비용 더 듦