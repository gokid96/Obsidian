# 주요 학습 방법

## 2. SFT (Supervised Fine-Tuning)

### 개념
사람이 만든 고품질 (질문, 답변) 쌍으로 "대화 방법"을 가르치는 단계

### 핵심 원리
- Input: "서울의 인구는?"
- Output: "서울의 인구는 약 950만 명입니다."
- 이런 쌍을 수만~수십만 개 학습

|항목|내용|
|---|---|
|데이터|사람이 직접 작성한 instruction-response 쌍|
|비용|Pre-training 대비 훨씬 저렴|
|결과물|Instruction-tuned Model|

### 장점
- 구현이 단순함
- 결과가 예측 가능함
### 한계
- 사람이 작성한 "정답"만 학습 → 창의성 부족
- "뭐가 더 좋은 답인지"는 모름

---

## 3. RLHF (Reinforcement Learning from Human Feedback)

### 개념

사람의 **선호도 피드백**을 기반으로 강화학습하는 단계

### 학습 과정 (3단계)

```
Step 1: 비교 데이터 수집
   질문 하나에 답변 A, B 생성
        ↓
   사람이 "A가 B보다 낫다" 판단
        ↓
Step 2: Reward Model 학습
   "어떤 답변이 좋은지" 점수 매기는 모델 학습
        ↓
Step 3: PPO로 최적화
   Reward Model 점수가 높아지도록 LLM 업데이트
```

|항목|내용|
|---|---|
|핵심 알고리즘|PPO (Proximal Policy Optimization)|
|필요한 것|Reward Model + 강화학습 인프라|
|사용처|GPT-4, Claude, Gemini 등 대부분의 상용 모델|

### 장점
- "정답"이 없는 주관적 품질도 학습 가능
- 안전성, 유용성 등 복합적인 기준 반영 가능
- 가장 검증된 alignment 방법

### 단점
- 구현 복잡도 높음
- Reward Model 학습 필요
- 학습 불안정할 수 있음
- 비용이 많이 듦

---

## 4. DPO (Direct Preference Optimization)

### 개념
RLHF의 간소화 버전. Reward Model 없이 선호도 데이터로 **직접** 학습

### 핵심 원리
```
RLHF: 선호도 데이터 → Reward Model 학습 → PPO로 LLM 학습
DPO:  선호도 데이터 → 바로 LLM 학습 (Reward Model 스킵)
```

### 특징

|항목|내용|
|---|---|
|논문|2023년 스탠포드에서 발표|
|필요한 것|선호도 데이터만 있으면 됨|
|사용처|Llama 3, Zephyr, 많은 오픈소스 모델|

### 장점

- 구현이 SFT만큼 단순
- 학습이 안정적
- 메모리/비용 효율적

### 단점

- RLHF만큼의 성능이 나오는지 논쟁 중
- 복잡한 선호도 반영에 한계가 있을 수 있음

---

## 5. LoRA / QLoRA

### 개념

전체 파라미터가 아닌 **일부만 학습**하는 효율적 파인튜닝 기법

### 핵심 원리
```
기존: 70B 파라미터 전체 업데이트 (VRAM 수백 GB 필요)
LoRA: 저차원 행렬만 추가해서 학습 (VRAM 수십 GB)
QLoRA: 모델을 4bit로 양자화 + LoRA (VRAM 10~20 GB)
```

|기법|VRAM 필요량 (7B 모델 기준)|특징|
|---|---|---|
|Full Fine-tuning|~60GB|최고 성능, 비용 높음|
|LoRA|~16GB|성능 약간 손해, 효율적|
|QLoRA|~6GB|개인 GPU로 가능|

### 사용 시점

- 개인/소규모 팀이 커스텀 모델 만들 때
- 특정 도메인 적응 (의료, 법률 등)
- RTX 3060 Ti로 7B 모델 QLoRA 가능!

---

## 6. 선택 기준 정리

### 어떤 상황에 뭘 써야 하나?

|상황|추천 방법|
|---|---|
|대화 형식만 가르치고 싶다|SFT|
|답변 품질/안전성 높이고 싶다|RLHF 또는 DPO|
|리소스가 충분하고 최고 품질 원함|RLHF|
|리소스 제한적, 간단하게 하고 싶다|DPO|
|개인 GPU로 파인튜닝 하고 싶다|QLoRA + SFT|
|특정 도메인 적응만 필요|LoRA/QLoRA + SFT|

### RLHF vs DPO 선택

|기준|RLHF|DPO|
|---|---|---|
|구현 난이도|높음|낮음|
|학습 안정성|불안정할 수 있음|안정적|
|성능|검증됨, 최고 수준|대부분 비슷, 논쟁 중|
|비용|높음|낮음|
|선택 기준|자원 충분 + 최고 품질|빠르고 효율적으로|

---

## 7. 추가로 알면 좋은 개념

|용어|설명|
|---|---|
|**Alignment**|모델이 인간 의도대로 동작하게 만드는 것|
|**Constitutional AI**|Anthropic 방식. AI가 스스로 피드백하며 학습|
|**RLAIF**|사람 대신 AI가 피드백 제공|
|**Instruction Tuning**|SFT의 다른 표현|
|**Post-training**|Pre-training 이후 모든 학습 (SFT + RLHF 등)|

---

이 정도면 LLM 학습 전반에 대한 핵심 개념이 정리될 거예요. 추가로 궁금한 부분 있으면 말씀해 주세