
|상황|선택|
|---|---|
|개인/소규모 팀|QLoRA (거의 100%)|
|회사에서 A100 있음|LoRA|
|빅테크급 리소스|Full Fine-tuning|

# Pre-training (사전학습)

## 개념
모델에게 "언어가 뭔지" 가르치는 첫 번째 단계

## 학습 방법

**Next Token Prediction (다음 토큰 예측)**

입력: "오늘 날씨가 정말"
정답: "좋다"

→ 모델이 "좋다"를 맞추도록 학습
→ 이걸 수조 번 반복

## 학습 데이터
- 웹페이지, 책, 논문, 뉴스, 위키피디아, 코드 등
- 인터넷에 있는 거의 모든 텍스트

## 결과물: Base Model

언어 패턴은 알지만 대화는 못함:

사용자: "서울의 수도는?"
Base Model: "서울의 수도는? 부산의 수도는? 대구의..."
(대화가 아니라 패턴 이어가기)

→ 그래서 Fine-tuning이 필수

## 비용

| 항목 | 규모 |
|------|------|
| 데이터 | 수조 개 토큰 |
| 비용 | 수백억 ~ 수천억 원 |
| 기간 | 수 주 ~ 수 개월 |
| 장비 | 수천 개의 GPU |

## 한 줄 요약
> "인터넷의 모든 글을 읽고 다음에 올 단어를 예측하는 연습을 수조 번 한다"
