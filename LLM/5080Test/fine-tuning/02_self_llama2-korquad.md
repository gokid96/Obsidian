## QLoRA Fine-tuning íŠœí† ë¦¬ì–¼ (Self ë²„ì „)

## ê°œìš”

|í•­ëª©|ë‚´ìš©|
|---|---|
|ëª©í‘œ|Llama-2-7Bë¥¼ KorQuAD ë°ì´í„°ë¡œ fine-tuning|
|ë°©ì‹|QLoRA (4bit ì–‘ìí™” + LoRA)|
|íŠ¹ì§•|ì§ì ‘ ì–‘ìí™” ì„¤ì • (BitsAndBytesConfig)|
|í™˜ê²½|Ubuntu + RTX 5080 (16GB VRAM)|

---

## QLoRAë€?
```
QLoRA = Quantization + LoRA

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ë² ì´ìŠ¤ ëª¨ë¸ (Llama-2-7B)             â”‚
â”‚ - FP16 â†’ 4bit ì–‘ìí™” (ë™ê²°)          â”‚ â† Q
â”‚                                     â”‚
â”‚ LoRA ì–´ëŒ‘í„° (16.7M params)           â”‚
â”‚ - FP16 ìœ ì§€ (í•™ìŠµ)                   â”‚ â† LoRA
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

|êµ¬ë¶„|ì¼ë°˜ Fine-tuning|QLoRA|
|---|---|---|
|í•™ìŠµ íŒŒë¼ë¯¸í„°|6.7B (100%)|16.7M (0.25%)|
|VRAM|~28GB|~7GB|
|ì €ì¥ ìš©ëŸ‰|~14GB|~67MB|

---

## íŒŒì¼ êµ¬ì¡°
```
~/ai-projects/llama2-korquad/
â”œâ”€â”€ KorQuAD_v1.0_dev.json          # ì›ë³¸ ë°ì´í„°
â”œâ”€â”€ prepare_tutorial_data.py       # ë°ì´í„° ì¤€ë¹„
â”œâ”€â”€ korquad_tutorial.json          # í•™ìŠµ ë°ì´í„° (20ê°œ)
â”œâ”€â”€ self_train_qlora.py            # í•™ìŠµ (Self ë²„ì „)
â”œâ”€â”€ self_train_qlora_test.py       # í…ŒìŠ¤íŠ¸
â””â”€â”€ llama2-korquad-qlora/
    â””â”€â”€ final/
        â””â”€â”€ adapter_model.safetensors
```

---

## í™˜ê²½ ì„¤ì •
```bash
python3 -m venv ~/ai-projects/venv
source ~/ai-projects/venv/bin/activate

pip install torch transformers accelerate
pip install bitsandbytes peft trl
pip install datasets
```

---

## 1. ë°ì´í„° ì¤€ë¹„ (prepare_tutorial_data.py)
```python
import json

with open("KorQuAD_v1.0_dev.json", "r") as f:
    data = json.load(f)

refined_dict = {}
for topic in data['data']:
    for para in topic['paragraphs']:
        for qa in para['qas']:
            refined_dict[qa['question']] = qa['answers'][0]['text']

samples = []
for idx, (q, a) in enumerate(refined_dict.items()):
    if idx >= 20:
        break
    prompt = f"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {q} ### Response: {a}"
    samples.append({"text": prompt})

with open("korquad_tutorial.json", "w", encoding="utf-8") as f:
    json.dump(samples, f, ensure_ascii=False, indent=2)

print(f"ìƒì„±: {len(samples)}ê°œ")
```

---

## 2. í•™ìŠµ (self_train_qlora.py)
```python
"""
Llama-2 QLoRA Fine-tuning - Self ë²„ì „ (ì§ì ‘ ì–‘ìí™”)
"""
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig

# ============================================
# ì„¤ì •
# ============================================
MODEL_NAME = "TinyPixel/Llama-2-7B-bf16-sharded"
OUTPUT_DIR = "./llama2-korquad-qlora"

# ============================================
# 4bit ì–‘ìí™” ì„¤ì •
# ============================================
print("âš™ï¸  4bit ì–‘ìí™” ì„¤ì •...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# ============================================
# ëª¨ë¸ ë¡œë“œ
# ============================================
print("ğŸ¤– ëª¨ë¸ ë¡œë”© (4bit ì–‘ìí™”)...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

# ============================================
# í† í¬ë‚˜ì´ì €
# ============================================
print("ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë”©...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# ============================================
# LoRA ì„¤ì •
# ============================================
print("ğŸ”§ LoRA ì„¤ì •...")
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)

model = get_peft_model(model, lora_config)
print("ğŸ“Š í•™ìŠµ íŒŒë¼ë¯¸í„°:")
model.print_trainable_parameters()

# ============================================
# ë°ì´í„°ì…‹ ë¡œë“œ
# ============================================
print("ğŸ“¦ ë°ì´í„°ì…‹ ë¡œë”©...")
dataset = load_dataset("json", data_files="korquad_tutorial.json", split="train")
print(f"  ë°ì´í„° ìˆ˜: {len(dataset)}")

# ============================================
# SFTConfig
# ============================================
sft_config = SFTConfig(
    output_dir=OUTPUT_DIR,
    num_train_epochs=100,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_strategy="epoch",
    optim="paged_adamw_8bit",
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    report_to="none",
    max_seq_length=512,
    dataset_text_field="text",
)

# ============================================
# SFTTrainer
# ============================================
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=dataset,
    processing_class=tokenizer,
)

trainer.train()

# ============================================
# ì €ì¥
# ============================================
print("ğŸ’¾ ëª¨ë¸ ì €ì¥...")
trainer.save_model(f"{OUTPUT_DIR}/final")
tokenizer.save_pretrained(f"{OUTPUT_DIR}/final")
print(f"âœ… ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/final")
```

---

## 3. í…ŒìŠ¤íŠ¸ (self_train_qlora_test.py)
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import torch

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

print("ğŸ¤– ëª¨ë¸ ë¡œë”©...")
base_model = AutoModelForCausalLM.from_pretrained(
    "TinyPixel/Llama-2-7B-bf16-sharded",
    quantization_config=bnb_config,
    device_map="auto",
)
model = PeftModel.from_pretrained(base_model, "./llama2-korquad-qlora/final")
tokenizer = AutoTokenizer.from_pretrained("TinyPixel/Llama-2-7B-bf16-sharded")

prompt_template = "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: %s ### Response: "

def gen(question):
    prompt = prompt_template % question
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=128, do_sample=False)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "")

questions = [
    ("ì„ì¢…ì„ì´ ì—¬ì˜ë„ ë†ë¯¼ í­ë ¥ ì‹œìœ„ë¥¼ ì£¼ë„í•œ í˜ì˜ë¡œ ì§€ëª…ìˆ˜ë°° ëœ ë‚ ì€?", "1989ë…„ 2ì›” 15ì¼"),
    ("1989ë…„ 6ì›” 30ì¼ í‰ì–‘ì¶•ì „ì— ëŒ€í‘œë¡œ íŒŒê²¬ ëœ ì¸ë¬¼ì€?", "ì„ìˆ˜ê²½"),
    ("ì„ì¢…ì„ì´ ì—¬ì˜ë„ ë†ë¯¼ í­ë ¥ ì‹œìœ„ë¥¼ ì£¼ë„í•œ í˜ì˜ë¡œ ì§€ëª…ìˆ˜ë°°ëœ ì—°ë„ëŠ”?", "1989ë…„"),
    ("ì„ì¢…ì„ì„ ê²€ê±°í•œ ì¥ì†ŒëŠ” ê²½í¬ëŒ€ ë‚´ ì–´ë””ì¸ê°€?", "í•™ìƒíšŒê´€ ê±´ë¬¼ ê³„ë‹¨"),
    ("ì„ì¢…ì„ì´ ì¡°ì‚¬ë¥¼ ë°›ì€ ë’¤ ì¸ê³„ëœ ê³³ì€ ì–´ë”˜ê°€?", "ì„œìš¸ì§€ë°©ê²½ì°°ì²­ ê³µì•ˆë¶„ì‹¤"),
]

print("\n" + "="*60)
print("ğŸ“ í…ŒìŠ¤íŠ¸ (5ê°œ ìƒ˜í”Œ)")
print("="*60)

correct = 0
for q, answer in questions:
    result = gen(q)
    match = "âœ…" if answer in result else "âŒ"
    if answer in result:
        correct += 1
    print(f"\nì§ˆë¬¸: {q[:50]}...")
    print(f"ì •ë‹µ: {answer}")
    print(f"ìƒì„±: {result[:50]}")
    print(f"ì¼ì¹˜: {match}")

print(f"\nì •í™•ë„: {correct}/{len(questions)}")
```

---

## ì‹¤í–‰
```bash
cd ~/ai-projects/llama2-korquad
source ~/ai-projects/venv/bin/activate

python prepare_tutorial_data.py   # ë°ì´í„° ì¤€ë¹„
python self_train_qlora.py        # í•™ìŠµ
python self_train_qlora_test.py   # í…ŒìŠ¤íŠ¸
```

---

## í•µì‹¬ í¬ì¸íŠ¸

1. **ì§ì ‘ ì–‘ìí™”** â€” BitsAndBytesConfigë¡œ 4bit ì–‘ìí™” ì„¤ì •
2. **prepare_model_for_kbit_training()** â€” ì–‘ìí™” ëª¨ë¸ í•™ìŠµ ì¤€ë¹„
3. **LoraConfig** â€” ìˆ˜ë™ìœ¼ë¡œ LoRA ì–´ëŒ‘í„° ì„¤ì •
4. **ì›ë¦¬ ì´í•´ì— ì í•©** â€” ê° ë‹¨ê³„ê°€ ëª…ì‹œì ìœ¼ë¡œ ë¶„ë¦¬ë¨